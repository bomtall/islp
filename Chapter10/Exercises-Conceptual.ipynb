{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 10.10 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. \n",
    "\n",
    "Consider a neural network with two hidden layers: $p = 4$ input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(a) Draw a picture of the network, similar to Figures 10.1 or 10.4\n",
    "\n",
    "#### Answer\n",
    "\n",
    "Using https://alexlenail.me/NN-SVG/index.html, we get:\n",
    "\n",
    "![](Exercises-Conceptual-1a-Network.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(b) Write out an expression for $f\\left(X\\right)$, assuming ReLU activation functions. Be as explicit as you can!\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f\\left(X\\right) \n",
    "&= \\underbrace{W_3}_{\\in\\mathbb{R}^{1\\times 3}}\\cdot\\underbrace{a_2}_{\\in\\mathbb{R}^{3\\times 1}} + \\underbrace{b_3}_{\\in\\mathbb{R^{1\\times 1}}}\\\\\n",
    "&= W_3\\cdot\\max\\left(0, \\underbrace{W_2}_{\\in\\mathbb{R}^{3\\times 2}}\\cdot\\underbrace{a_1}_{\\in\\mathbb{R}^{2\\times 1}} + \\underbrace{b_2}_{\\in\\mathbb{R}^{3\\times 1}}\\right) + b_3\\\\\n",
    "&= W_3\\cdot\\max\\left(0, W_2\\cdot\\max\\left(0,\\underbrace{W_1}_{\\in\\mathbb{R}^{2\\times 4}}\\cdot \\underbrace{X}_{\\in\\mathbb{R}^{4\\times 1}} + \\underbrace{b_1}_{\\in\\mathbb{R}^{2\\times 1}}\\right) + b_2\\right) + b_3\\\\\n",
    "&= W_3\\cdot\\max\\left(0, W_2\\cdot\\max\\left(0,W_1\\cdot X + b_1\\right) + b_2\\right) + b_3\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(c) Now plug in some values for the coefficients and write out the value of $f\\left(X\\right)$.\n",
    "\n",
    "#### Answer\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) How many parameters are there?\n",
    "\n",
    "#### Answer\n",
    "\n",
    "- $W_3\\in\\mathbb{R}^{1\\times 3}, b_3\\in\\mathbb{R}^{1\\times 1}$ - 4 parameters\n",
    "- $W_2\\in\\mathbb{R}^{3\\times 2}, b_2\\in\\mathbb{R}^{3\\times 1}$ - 9 parameters\n",
    "- $W_1\\in\\mathbb{R}^{2\\times 4}, b_1\\in\\mathbb{R}^{2\\times 1}$ - 10 parameters\n",
    "\n",
    "Therefore, there are 23 parameters in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2. \n",
    "\n",
    "Consider the *softmax* function in (10.13) (see also (4.13) on page 145) for modeling multinomial probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) In (10.13), show that if we add a constant $c$ to each of the $Z_l$, then the probability is unchanged.\n",
    "\n",
    "#### Answer\n",
    "\n",
    "For (10.13), we have:\n",
    "\n",
    "$$\n",
    "f_{m}\\left(X\\right) = \\mathbb{P}\\left[Y = m \\middle| X\\right] = \\frac{e^{Z_m}}{\\sum_{l=0}^9e^{Z_l}}\n",
    "$$\n",
    "\n",
    "Adding constant $c$ to each $Z_l$ we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_{m,c}\\left(X\\right)\n",
    "&= \\frac{e^{Z_m + c}}{\\sum_{l=0}^9e^{Z_l + c}}\\\\\n",
    "&= \\frac{e^c\\cdot e^{Z_m}}{\\sum_{l=0}^9e^c\\cdot e^{Z_l}}\\\\\n",
    "&= \\frac{e^{Z_m}}{\\sum_{l=0}^9e^{Z_l}}\\\\\n",
    "&= f_m\\left(X\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(b) In (4.13), show that if we add constants $c_j$ , $j = 0, 1,\\dots,p$, to each of the corresponding coefficients for each of the classes, then the predictions at any new point $x$ are unchanged.\n",
    "\n",
    "#### Answer\n",
    "\n",
    "We will define $\\tilde{x} = \\begin{bmatrix}1 & x\\end{bmatrix}$\n",
    "\n",
    "In (4.13), we have:\n",
    "$$\n",
    "\\mathbb{P}\\left[Y=k\\middle|X = x\\right] = \\frac{\\exp\\left(\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p\\right)}{\\sum_{l=1}^K\\exp\\left(\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p\\right)} = \\frac{\\exp\\left(\\beta_{k}'\\tilde{x}\\right)}{\\sum_{l=1}^K\\exp\\left(\\beta_{l}'\\tilde{x}\\right)}\n",
    "$$\n",
    "\n",
    "Adding $c$ to each coefficient gets us:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\exp\\left(\\left(\\beta_{k} + c\\right)'\\tilde{x}\\right)}{\\sum_{l=1}^K\\exp\\left(\\left(\\beta_{l} + c\\right)'\\tilde{x}\\right)}\n",
    "&= \\frac{\\exp\\left(c'\\tilde{x}\\right)\\cdot\\exp\\left(\\beta_{k}'\\tilde{x}\\right)}{\\exp\\left(c'\\tilde{x}\\right)\\cdot\\sum_{l=1}^K\\exp\\left(\\beta_{l}'\\tilde{x}\\right)}\\\\\n",
    "&= \\frac{\\exp\\left(\\beta_{k}'\\tilde{x}\\right)}{\\sum_{l=1}^K\\exp\\left(\\beta_{l}'\\tilde{x}\\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- This shows that softmax is overparameterized which means that the number of parameters which need to be trained is much larger than the number of training samples available.\n",
    "- However, regularization and stochastic gradient descent helps constrain the solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 3.\n",
    "\n",
    "Show that the negative multinomial log-likelihood (10.14) is equivalent to the negative log of the likelihood expression (4.5) when there are $M = 2$ classes.\n",
    "\n",
    "#### Answer\n",
    "\n",
    "Using (10.14) for $M=2$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\sum_{i=1}^n\\left(y_{i0}\\log\\left(f_0\\left(x_i\\right)\\right) + y_{i1}\\log\\left(f_1\\left(x_i\\right)\\right)\\right)\n",
    "&= -\\sum_{i=1}^n\\left(\\log\\left(f_0\\left(x_i\\right)^{y_{i0}}\\right) + \\log\\left(f_1\\left(x_i\\right)^{y_{i1}}\\right)\\right)\\\\\n",
    "&= -\\sum_{i=1}^n\\log\\left(f_0\\left(x_i\\right)^{y_{i0}}\\cdot f_1\\left(x_i\\right)^{y_{i1}}\\right)\\\\\n",
    "&= -\\log\\left(\\prod_{i=1}^nf_0\\left(x_i\\right)^{y_{i0}}\\cdot f_1\\left(x_i\\right)^{y_{i1}}\\right)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
